{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae9376c-3c19-449a-b441-2ed268fc9a9f",
   "metadata": {},
   "source": [
    "# Segmentation Model\n",
    "\n",
    "This is copied and modified from the example. I changed the colors of the masks for our data and changed the picture size. Someone should go through this and determine what is happening, and what to keep. There are spelling errors throughout the text, which could be fixed. Trying new models is relatively easy -- there are very few places to change. Most of the code here is for augmentation, visualization, and loading data into the model. The actual model structures are in the segmentation_models_pytorch library, and choice of model is easy to choose and change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1c623-d480-48c3-b008-3519b0685bc7",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167e285-69a0-4f16-b830-d1e984e97845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "# the list should be updated based on what is needed\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torch\n",
    "import random \n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils\n",
    "import albumentations as albu\n",
    "\n",
    "import torch_directml\n",
    "import os\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74333da1-b1de-4dc7-9139-f023e1215fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part of the code is to seed the rng, to give consistent results for demostration\n",
    "# likely, this can be removed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08638f-b8f5-4da5-8b76-325c66f73c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "# note that the removal of the color text means that the black / white colors need to be added manually\n",
    "\n",
    "DATSET_NAME = \"Dataset\"\n",
    "\n",
    "X_TRAIN_DIR = f\"{DATSET_NAME}/Train_Photo\"\n",
    "Y_TRAIN_DIR = f\"{DATSET_NAME}/Train_Annot\"\n",
    "\n",
    "X_VALID_DIR = f\"{DATSET_NAME}/Val_Photo\"\n",
    "Y_VALID_DIR = f\"{DATSET_NAME}/Val_Annot\"\n",
    "\n",
    "X_TEST_DIR = f\"{DATSET_NAME}/Val_Photo\"\n",
    "Y_TEST_DIR = f\"{DATSET_NAME}/Val_Annot\"\n",
    "\n",
    "#LABEL_COLORS_FILE = f\"{DATSET_NAME}/label_colors.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d9128e-26cf-439b-b4c4-07d2e0c11174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# I need to copy this lower for additional models\n",
    "\n",
    "CLASSES = [\n",
    "    \"background\",\n",
    "    \"road\"\n",
    "]\n",
    "\n",
    "# https://segmentation-models-pytorch.readthedocs.io/en/latest/encoders.html\n",
    "ENCODER = 'resnet18'\n",
    "# based on encoder, typically imagenet\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "# “sigmoid”, “softmax”, “logsoftmax”, “tanh”, “identity”, callable and None\n",
    "ACTIVATION = 'softmax2d' \n",
    "# directml\n",
    "#DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = torch_directml.device()\n",
    "\n",
    "EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "INIT_LR = 0.0005\n",
    "LR_DECREASE_STEP = 15\n",
    "LR_DECREASE_COEF = 2 # LR будет разделен на этот коэф раз в LR_DECREASE_STEP эпох\n",
    "\n",
    "# if this is the image size, it needs to be changed\n",
    "INFER_WIDTH = 960\n",
    "INFER_HEIGHT = 544\n",
    "\n",
    "# https://segmentation-models-pytorch.readthedocs.io/en/latest/losses.html\n",
    "loss = utils.losses.DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4dcbeb-5a52-4b1e-8870-6e9759bed6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be updated for a single target region\n",
    "\n",
    "def create_annotations_of_no_calsses_images(images_folder, annotations_folder):\n",
    "    image_files = os.listdir(images_folder)\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(images_folder, image_file)\n",
    "        annotation_path = os.path.join(annotations_folder, f\"{os.path.splitext(image_file)[0]}.png\")        \n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        \n",
    "        if not os.path.exists(annotation_path):\n",
    "            img = Image.new('RGB', (width, height), color='black')\n",
    "            img.save(annotation_path)\n",
    "\n",
    "def _convert_multichannel2singlechannel(mc_mask: np.ndarray):\n",
    "    \"\"\" Осуществляет перевод трехканальной маски (число каналов сколько классов) в трехканальное \n",
    "    изображение где будет расцветка как зададим в словаре colors_imshow для классов \"\"\"\n",
    "\n",
    "    colors_imshow = {\n",
    "            \"background\" : np.array([0, 0, 0]),\n",
    "            \"road\" : np.array([255, 255, 255]),\n",
    "    }\n",
    "\n",
    "    sc_mask = np.zeros((mc_mask[0].shape[0], mc_mask[0].shape[1], 2), dtype=np.uint8)\n",
    "    square_ratios = {}\n",
    "\n",
    "    for i, singlechannel_mask in enumerate(mc_mask):\n",
    "\n",
    "        cls = CLASSES[i]\n",
    "        singlechannel_mask = singlechannel_mask.squeeze()\n",
    "\n",
    "        # Заодно осуществляет подсчет процента каждого класса (сумма пикселей на общее число)\n",
    "        square_ratios[cls] = singlechannel_mask.sum() / singlechannel_mask.size\n",
    "        \n",
    "        sc_mask += np.multiply.outer(singlechannel_mask > 0, colors_imshow[cls]).astype(np.uint8)\n",
    "        \n",
    "\n",
    "    title = \"Площади: \" + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in CLASSES])\n",
    "    return sc_mask, title\n",
    "\n",
    "\n",
    "def visualize_multichennel_mask(img: np.ndarray, multichennel_mask: np.ndarray):\n",
    "    \"\"\" Реализация демонстрации маски и самого изображения \"\"\"\n",
    "    # размер маски: H, W, CHANNEL\n",
    "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(img)\n",
    "    multichennel_mask = multichennel_mask.transpose(2, 0, 1)\n",
    "    mask_to_show, title = _convert_multichannel2singlechannel(multichennel_mask)\n",
    "    axes[1].imshow(mask_to_show)\n",
    "    axes[1].set_title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9405c-a0e1-46a3-8780-8a1462953a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        masks_dir,\n",
    "        augmentation=None,\n",
    "        preprocessing=None\n",
    "    ):\n",
    "        self.images_paths = glob(f\"{images_dir}/*\")\n",
    "        self.masks_paths = glob(f\"{masks_dir}/*\")\n",
    "\n",
    "        # our colors are no longer in a file\n",
    "        self.cls_colors = self._get_classes_colors()#LABEL_COLORS_FILE)\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    # remove the references to a file\n",
    "    def _get_classes_colors(self): #, label_colors_dir):\n",
    "        #cls_colors = {}\n",
    "        #with open(label_colors_dir) as file:\n",
    "        #    while line := file.readline():\n",
    "        #        R, G, B, label = line.rstrip().split()\n",
    "        #        cls_colors[label] = np.array([B, G, R], dtype=np.uint8)\n",
    "\n",
    "        keyorder = CLASSES\n",
    "        cls_colors_ordered = {}\n",
    "        for k in keyorder:\n",
    "            if k==\"road\":\n",
    "                cls_colors_ordered[k] = np.array([255, 255, 255], dtype=np.uint8)\n",
    "            elif k==\"background\":\n",
    "                cls_colors_ordered[k] = np.array([0, 0, 0], dtype=np.uint8)\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected label {k}, cls colors: {cls_colors}\")\n",
    "\n",
    "        return cls_colors_ordered\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = cv2.imread(self.images_paths[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(self.masks_paths[i])\n",
    "        masks = [cv2.inRange(mask, color, color) for color in self.cls_colors.values()]\n",
    "        masks = [(m > 0).astype(\"float32\") for m in masks]\n",
    "        mask = np.stack(masks, axis=-1).astype(\"float\")\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6874e-f701-46b9-bb1c-2fa02f228c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(X_TRAIN_DIR, Y_TRAIN_DIR)\n",
    "image, mask = dataset[np.random.randint(len(dataset))]\n",
    "visualize_multichennel_mask(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24787c4e-cf38-423e-b337-6d953b134142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.LongestMaxSize(max_size=INFER_HEIGHT, always_apply=True),\n",
    "        albu.PadIfNeeded(min_height=int(INFER_HEIGHT*1.1), min_width=int(INFER_WIDTH*1.1), border_mode=2, always_apply=True),\n",
    "        albu.RandomCrop(height=INFER_HEIGHT, width=INFER_WIDTH, always_apply=True),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.Sharpen(alpha=(0.1, 0.2), lightness=(0.1, 0.2), p=0.5),  # Уменьшение вероятности и уменьшение интенсивности\n",
    "                albu.Blur(blur_limit=[1, 3], p=0.5),  # Уменьшение вероятности и уменьшение интенсивности\n",
    "                albu.GaussNoise(var_limit=(1, 5), p=0.5),  # Уменьшение вероятности и уменьшение интенсивности\n",
    "            ],\n",
    "            p=0.7,  # Уменьшение вероятности применения любой аугментации\n",
    "        ),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),  # Уменьшение вероятности и уменьшение интенсивности\n",
    "                albu.HueSaturationValue(hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=5, p=0.5),  # Уменьшение вероятности и уменьшение интенсивности\n",
    "                albu.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.5),  # Уменьшение вероятности и уменьшение интенсивности\n",
    "            ],\n",
    "            p=0.7,  # Уменьшение вероятности применения любой аугментации цвета\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [albu.LongestMaxSize(max_size=INFER_HEIGHT, always_apply=True),\n",
    "    albu.PadIfNeeded(min_height=INFER_HEIGHT, min_width=INFER_WIDTH, border_mode=2, always_apply=True),\n",
    "    albu.CenterCrop(height=INFER_HEIGHT, width=INFER_WIDTH, always_apply=True)]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    # Осуществит стартовую нормализацию данных согласно своим значениям или готовым для imagenet\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159579f-6ad3-4e1d-8106-59c7c68bf9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = Dataset(\n",
    "    X_TRAIN_DIR, \n",
    "    Y_TRAIN_DIR, \n",
    "    augmentation=get_training_augmentation()\n",
    ")\n",
    "\n",
    "# same image with different random transforms\n",
    "indx = np.random.randint(len(augmented_dataset))\n",
    "\n",
    "for i in range(2):\n",
    "    image, mask = augmented_dataset[indx]\n",
    "    visualize_multichennel_mask(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82bf16-4407-432b-b507-c84b6cd7aed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = Dataset(\n",
    "    X_VALID_DIR, \n",
    "    Y_VALID_DIR, \n",
    "    augmentation=get_validation_augmentation()\n",
    ")\n",
    "\n",
    "indx = np.random.randint(len(augmented_dataset))\n",
    "\n",
    "image, mask = augmented_dataset[indx]\n",
    "visualize_multichennel_mask(image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0f5df-90a6-457f-9cdb-55c4a1387d3f",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "104ad987-3ad4-4211-aed5-57550e30cffb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# https://segmentation-models-pytorch.readthedocs.io/en/latest/encoders.html\n",
    "#ENCODER = 'resnet18'\n",
    "# based on encoder, typically imagenet\n",
    "#ENCODER_WEIGHTS = 'imagenet'\n",
    "# “sigmoid”, “softmax”, “logsoftmax”, “tanh”, “identity”, callable and None\n",
    "#ACTIVATION = 'softmax2d' \n",
    "\n",
    "# https://segmentation-models-pytorch.readthedocs.io/en/latest/losses.html\n",
    "#loss = utils.losses.DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0348d19-68a3-40ef-bc9a-70f2e9094a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9216a-b106-40c2-b617-503bb09396c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "#preprocessing_fn = lambda img, **kwargs: img.astype(\"float32\") / 255    # -- классика деление на 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d8ee1-daf5-4bd8-a6c2-a0b6a76d199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    X_TRAIN_DIR, \n",
    "    Y_TRAIN_DIR, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    X_VALID_DIR, \n",
    "    Y_VALID_DIR, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f01ec0-615e-407b-87de-44bca82a03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    utils.metrics.Fscore(),\n",
    "    utils.metrics.IoU()\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=INIT_LR),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0d501-ed32-4b42-a16b-97c86009c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f942242-a028-4802-9923-98531ad1409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "\n",
    "loss_logs = {\"train\": [], \"val\": []}\n",
    "metric_logs = {\"train\": [], \"val\": []}\n",
    "for i in range(0, EPOCHS):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    train_loss, train_metric, train_metric_IOU = list(train_logs.values())\n",
    "    loss_logs[\"train\"].append(train_loss)\n",
    "    metric_logs[\"train\"].append(train_metric_IOU)\n",
    "\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    val_loss, val_metric, val_metric_IOU = list(valid_logs.values())\n",
    "    loss_logs[\"val\"].append(val_loss)\n",
    "    metric_logs[\"val\"].append(val_metric_IOU)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, 'models/best_model_new.pth')\n",
    "        # Save the model with JIT\n",
    "        # Create a tensor with the specified dimensions\n",
    "        trace_image = torch.randn(BATCH_SIZE, 3, INFER_HEIGHT, INFER_WIDTH)\n",
    "        # Trace the model using the example input\n",
    "        traced_model = torch.jit.trace(model, trace_image.to(DEVICE))\n",
    "        torch.jit.save(traced_model, 'models/best_model_new.pt')\n",
    "        print('Model saved!')\n",
    "\n",
    "    print(\"LR:\", optimizer.param_groups[0]['lr'])\n",
    "    if i > 0 and i % LR_DECREASE_STEP == 0:\n",
    "        print('Decrease decoder learning rate')\n",
    "        optimizer.param_groups[0]['lr'] /= LR_DECREASE_COEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3ff37-db47-4985-a22e-5634b876facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10,4))\n",
    "axes[0].plot(loss_logs[\"train\"], label = \"train\")\n",
    "axes[0].plot(loss_logs[\"val\"], label = \"val\")\n",
    "axes[0].set_title(\"losses - Dice\")\n",
    "\n",
    "axes[1].plot(metric_logs[\"train\"], label = \"train\")\n",
    "axes[1].plot(metric_logs[\"val\"], label = \"val\")\n",
    "axes[1].set_title(\"IOU\")\n",
    "\n",
    "[ax.legend() for ax in axes];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69b00-170b-4c82-975f-15d9d4726fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "#best_model = torch.load('models/best_model_new.pth')\n",
    "best_model = torch.jit.load('models/best_model_new.pt', map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed3e31-e97b-492a-8566-77aebfabff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_augmentation():\n",
    "    test_transform = [albu.LongestMaxSize(max_size=INFER_HEIGHT, always_apply=True),\n",
    "    albu.PadIfNeeded(min_height=INFER_HEIGHT, min_width=INFER_WIDTH, border_mode=0, always_apply=True),\n",
    "    albu.CenterCrop(height=INFER_HEIGHT, width=INFER_WIDTH, always_apply=True)]\n",
    "    return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e240b57-8547-48ea-9c7d-1f4c93daeec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# оценка IoU\n",
    "test_dataset = Dataset(\n",
    "    X_TEST_DIR, \n",
    "    Y_TEST_DIR, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "logs = test_epoch.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f505f2-9146-4309-9ce8-3eec10e172b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_imshow = {\n",
    "        \"background\" : np.array([0, 0, 0]),\n",
    "        \"road\" : np.array([255, 255, 255]),\n",
    "    }\n",
    "\n",
    "\n",
    "def _colorize_mask(mask: np.ndarray):\n",
    "    mask = mask.squeeze()\n",
    "    colored_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    square_ratios = {}\n",
    "    for cls_code, cls in enumerate(CLASSES):\n",
    "        cls_mask = mask == cls_code\n",
    "        square_ratios[cls] = cls_mask.sum() / cls_mask.size\n",
    "        colored_mask += np.multiply.outer(cls_mask, colors_imshow[cls]).astype(np.uint8)\n",
    "\n",
    "    return colored_mask, square_ratios\n",
    "\n",
    "\n",
    "def reverse_normalize(img, mean, std):\n",
    "    # Invert normalization\n",
    "    img = img * np.array(std) + np.array(mean)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_predicts(img: np.ndarray, mask_gt: np.ndarray, mask_pred: np.ndarray, normalized=False):\n",
    "    # размер img: H, W, CHANNEL\n",
    "    # размер mask_gt, mask_pred: H, W, значения - range(len(CLASSES)\n",
    "    _, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    if normalized:\n",
    "        # Reverse the normalization to get the unnormalized image\n",
    "        img = reverse_normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    axes[0].imshow(img)\n",
    "\n",
    "    mask_gt, square_ratios = _colorize_mask(mask_gt)\n",
    "    title = \"Площади:\\n\" + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in CLASSES])\n",
    "    axes[1].imshow(mask_gt, cmap=\"twilight\")\n",
    "    axes[1].set_title(f\"GT маска\\n\" + title)\n",
    "\n",
    "    mask_pred, square_ratios = _colorize_mask(mask_pred)\n",
    "    title = \"Площади:\\n\" + \"\\n\".join([f\"{cls}: {square_ratios[cls]*100:.1f}%\" for cls in CLASSES])\n",
    "    axes[2].imshow(mask_pred, cmap=\"twilight\")\n",
    "    axes[2].set_title(f\"PRED маска\\n\" + title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2ae25-b275-4131-b3ad-67e6cd8a9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image, gt_mask = test_dataset[n]\n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model(x_tensor)\n",
    "    pr_mask = pr_mask.squeeze().cpu().detach().numpy()\n",
    "    \n",
    "    label_mask = np.argmax(pr_mask, axis=0)\n",
    "    print(label_mask.shape, image.shape, gt_mask.shape)\n",
    "\n",
    "    visualize_predicts(image, np.argmax(gt_mask, axis=0), label_mask, normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449c6a47-df77-417f-8b0b-b2837d225e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413e67a-9a81-469f-8cfb-d7231bd90e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
